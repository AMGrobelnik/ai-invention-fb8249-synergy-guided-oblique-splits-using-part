{
  "title": "SG-FIGS Spec",
  "summary": "Comprehensive technical specification for implementing Synergy-Guided Oblique FIGS (SG-FIGS), covering: dit PID API with exact synergy extraction keys ((0,1),), FIGS source code modification points (Node extension, _construct_node_with_stump replacement, predict modification), complete RO-FIGS source code analysis with SPyCT/ODT oblique split construction and L1/2 regularization, synergy graph construction algorithm (discretize, pairwise PID, threshold, networkx cliques), and evaluation protocol with 22 OpenML datasets. Saved to resources/sg_figs_technical_specification.md (626 lines, 17KB).",
  "answer": "# Technical Specification for Synergy-Guided Oblique FIGS (SG-FIGS)\n\n## 1. PID Library Selection and API Specification\n\n### 1.1 Recommended Library: dit\n\nThe `dit` library (discrete information theory) is the recommended choice for computing pairwise Partial Information Decomposition (PID) synergy [1, 2]. The library implements the Williams & Beer framework with multiple PID measures including `PID_WB`, `PID_BROJA`, `PID_CCS`, `PID_GK`, and others [3].\n\n**Why dit over alternatives:**\n- **pidpy**: Only supports binary features (0/1) for arbitrary numbers of variables; non-binary integer variables are limited to triplets only [4]. This makes pidpy unsuitable for discretized continuous features with >2 bins.\n- **PIDF** (from Westphal et al., AISTATS'25): Uses a custom neural mutual information estimation (MINE) approach rather than classical PID [5, 6]. While scalable, it does not provide the exact PID decomposition needed for synergy graph construction.\n- **JIDT** (Java): Requires jpype bridge; useful as fallback if dit is too slow for >50 features [2].\n\n### 1.2 Exact API for Synergy Computation\n\n**Installation:** `uv pip install dit`\n\n**Core API pattern for bivariate PID:**\n\n```python\nimport dit\nfrom dit.pid import PID_WB  # or PID_BROJA\n\n# Create distribution from tuple outcomes (feature_i, feature_j, target)\nd = dit.Distribution(['000', '011', '102', '113'], [1/4]*4)\n\n# Compute bivariate PID with two source variables and one target\nresult = PID_WB(d)  # or PID_BROJA(d)\n```\n\n**Node key format in the bivariate lattice** (tuples of tuples) [3, 7]:\n- `{0:1}` = `((0, 1),)` = **Synergy** (joint-only info)\n- `{0}` = `((0,),)` = Unique info from X_0\n- `{1}` = `((1,),)` = Unique info from X_1\n- `{0}{1}` = `((0,), (1,))` = Redundancy (shared info)\n\n**Programmatic synergy extraction** [7]:\n\n```python\n# Method 1: Dictionary-style access via __getitem__\nsynergy = result[((0, 1),)]  # calls get_pi internally\n\n# Method 2: Explicit get_pi method\nsynergy = result.get_pi(((0, 1),))\n\n# Method 3: Get redundancy\nredundancy = result.get_red(((0,), (1,)))\n```\n\nThe `get_pi` method performs Moebius inversion: `pi(node) = red(node) - sum(pi(descendant) for descendant in lattice.descendants(node))` [7].\n\n**Constructing distributions from empirical data** [8]:\n\n```python\nfrom collections import Counter\nimport dit\nfrom dit.pid import PID_BROJA\n\ndef compute_pairwise_synergy(xi_discrete, xj_discrete, y_discrete):\n    triples = list(zip(xi_discrete, xj_discrete, y_discrete))\n    counts = Counter(triples)\n    total = len(triples)\n    outcomes = [str(a) + str(b) + str(c) for (a, b, c) in counts.keys()]\n    pmf = [v / total for v in counts.values()]\n    d = dit.Distribution(outcomes, pmf)\n    result = PID_BROJA(d)\n    return result.get_pi(((0, 1),))  # synergy value in bits\n```\n\n### 1.3 PID Measure Choice: PID_WB vs PID_BROJA\n\n**PID_WB (Williams & Beer I_min):** Known to over-attribute information as redundant in some cases. For the concatenation distribution, I_min incorrectly assigns 1 bit redundancy and 1 bit synergy instead of 2 bits unique information [3].\n\n**PID_BROJA (Bertschinger-Rauh-Olbrich-Jost-Ay):** Defines unique information via constrained optimization over distributions with fixed input-output marginals. Generally considered more principled for bivariate cases, but can produce unintuitive results on some distributions (e.g., reduced or) [3, 9].\n\n**Recommendation:** Use PID_BROJA as primary (more principled synergy estimates) with PID_WB as sensitivity analysis. Both are available in dit via `from dit.pid import PID_BROJA, PID_WB` [3].\n\n### 1.4 Validation Distributions\n\n- **XOR distribution** (`bivariates['synergy']`): Should yield synergy ~= 1.0 bit, redundancy ~= 0.0 [3]\n- **Redundant distribution** (`bivariates['redundant']`): Should yield synergy ~= 0.0, redundancy ~= 1.0 [3]\n- **Unique information** (`bivariates['cat']`): Should yield unique_0 ~= 1.0, unique_1 ~= 1.0 [3]\n\n## 2. Discretization Pipeline\n\n### 2.1 sklearn KBinsDiscretizer\n\n```python\nfrom sklearn.preprocessing import KBinsDiscretizer\n\ndiscretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\nX_discrete = discretizer.fit_transform(X).astype(int)\n```\n\n**Bin count analysis** [10]:\n- B=5: 5x5xn_classes = 50-125 joint states per pair -> fast (~0.01-0.05s per pair)\n- B=8: 8x8xn_classes = 128-512 joint states -> moderate (~0.05-0.2s per pair)\n- B=10: 10x10xn_classes = 200-1000 joint states -> upper limit (~0.1-0.5s per pair)\n\n### 2.2 Computational Budget for Pairwise Synergy\n\nNumber of pairs = d(d-1)/2:\n- d=10 -> 45 pairs x ~0.05s = ~2s\n- d=30 -> 435 pairs x ~0.05s = ~22s\n- d=50 -> 1,225 pairs x ~0.05s = ~61s\n- d=100 -> 4,950 pairs x ~0.05s = ~4 min\n\nThese estimates assume B=5, binary classification. PID_BROJA may be ~2-5x slower than PID_WB due to optimization [3].\n\n## 3. FIGS Source Code Analysis and Modification Points\n\n### 3.1 Original FIGS Node Class (imodels/tree/figs.py) [11]\n\n```python\nclass Node:\n    def __init__(self, feature: int = None, threshold: int = None,\n                 value=None, value_sklearn=None, idxs=None,\n                 is_root: bool = False, left=None,\n                 impurity: float = None, impurity_reduction: float = None,\n                 tree_num: int = None, node_id: int = None,\n                 right=None, depth=None):\n        self.feature = feature          # Single feature index\n        self.threshold = threshold      # Single threshold value\n        self.value = value\n        self.impurity_reduction = impurity_reduction\n        self.left = left; self.right = right\n        self.is_root = is_root; self.idxs = idxs\n```\n\n### 3.2 Required Node Extension for Oblique Splits\n\nBased on the RO-FIGS implementation [13], the Node class must be extended:\n\n```python\nclass Node:\n    def __init__(self, features=None,     # List of feature indices\n                 weights=None,            # Weight vector for linear combination\n                 threshold: int = None,   # Threshold for oblique hyperplane\n                 value=None, idxs=None, left=None, right=None,\n                 impurity: float = None, impurity_reduction: float = None,\n                 is_root: bool = False, tree_num: int = None):\n        self.features = features    # np.array of feature indices\n        self.weights = weights      # np.array of weights\n        self.threshold = threshold  # scalar threshold\n```\n\n### 3.3 Four Key Modification Points\n\n**Point 1: Replace _construct_node_with_stump with oblique split constructor** [11, 13]\n\nThe original method fits `DecisionTreeRegressor(max_depth=1)` for axis-aligned splits. Replace with oblique split using SPyCT ODT:\n```python\nstump = odt.Model(max_features=beam_size,\n                  splitting_features=splitting_features,\n                  random_state=self.random_state)\nstump.fit(X[idxs], y_regr[idxs])\n```\n\n**Point 2: Modify predict to handle oblique nodes** [13]\nOriginal: `x[root.feature] <= root.threshold`\nReplace: `np.dot(x[root.features], root.weights) <= root.threshold`\n\n**Point 3: Add synergy pre-computation at start of fit()**\nInsert before the main loop:\n```python\nself.synergy_graph_ = self._build_synergy_graph(X, y)\nself.synergy_subsets_ = self._extract_feature_subsets(self.synergy_graph_)\n```\n\n**Point 4: Modify feature subset selection in main loop**\nReplace `random.sample(range(d), beam_size)` with synergy-guided selection.\n\n### 3.4 FIGS Fit Loop Structure [11]\n\n1. Initialize: Construct first stump on full dataset\n2. Main while loop: While potential_splits not empty and not finished:\n   a. Pop node with max impurity_reduction\n   b. Check stopping conditions (min_impurity_decrease, max_trees, max_depth)\n   c. If root: start new tree, add placeholder new root to potential_splits\n   d. Add children to potential_splits\n   e. Recompute residuals: For each tree, subtract predictions of ALL other trees\n   f. Recompute all potential splits: Re-fit stumps on updated residuals\n   g. Sort potential_splits by impurity_reduction\n\n## 4. RO-FIGS Algorithm Details\n\n### 4.1 Complete Algorithm (from arxiv 2504.06927) [14]\n\nAlgorithm 1: RO-FIGS\nInput: X (features), y (outcomes), beam_size, min_imp_dec, max_splits\n\ntrees = []\nwhile (max_imp_dec > min_imp_dec OR first_iteration) AND total_splits < max_splits:\n    for repetition in range(r=5):\n        feat = select_random(beam_size)\n        for tree in all_trees:\n            y_res = y - predict(all_trees except tree)\n            for leaf in tree:\n                phi = compute_linear_combination(X, y_res, feat)\n                potential_splits.append(define_oblique_split(phi))\n        best_split = argmax(impurity_decrease, potential_splits)\n        if impurity_decrease(best_split) > min_imp_dec:\n            break\n    trees.insert(best_split)\nReturn: trees\n\n### 4.2 SPyCT/ODT Integration [13, 15, 16]\n\nThe RO-FIGS ODT module wraps SPyCT's GradSplitter:\n\n```python\nmodel = odt.Model(\n    splitter='grad',       # Gradient-based split optimization\n    max_depth=1,           # Single split (stump)\n    num_trees=1,\n    max_features=beam_size,\n    max_iter=100,          # Gradient descent iterations\n    lr=0.1,                # Learning rate\n    C=10,                  # Regularization (strength = 1/C)\n    tol=1e-2,\n    eps=1e-8,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    min_examples_to_split=5,\n    min_impurity_decrease=0.05,\n    splitting_features=splitting_features\n)\n```\n\n**How splitting_features works in _grow_tree** [15]:\nWhen splitting_features is provided, the code copies the list, appends a bias column index, converts to numpy array, and sorts. This directly restricts which features the GradSplitter considers during optimization.\n\n### 4.3 L_1/2 Regularization [14]\n\nThe split optimization objective is:\n\nmin_{w,b} ||w||_{1/2} + C * g(w,b)\n\nwhere:\n- ||w||_{1/2} = (sum_{i=1}^{k} sqrt(|w_i|))^2 -- the L_1/2 quasi-norm\n- C -- regularization strength (in SPyCT: regularization = 1/C)\n- g(w,b) -- fitness function minimizing weighted variance impurity on both sides of the hyperplane\n- k -- number of features (= beam_size)\n\nThe L_1/2 norm induces weight sparsity, meaning the actual number of non-zero feature weights per split is typically much less than beam_size [14].\n\n### 4.4 RO-FIGS Node Structure [13]\n\nThe RO-FIGS Node stores:\n- features: numpy array of feature indices used in the split\n- weights: numpy array of learned weights\n- threshold: scalar threshold for the hyperplane\n- impurity_reduction: impurity reduction from this split\n- value: prediction value (for leaves, or aggregate at split)\n\nPrediction for a single point [13]:\n```python\nif isinstance(root.features, int):\n    left = x[root.features] * root.weights <= root.threshold\nelse:\n    projection = sum(x[root.features[i]] * root.weights[i] for i in range(beam_size))\n    left = projection <= root.threshold\n```\n\n### 4.5 Hyperparameter Grid [14]\n\nRO-FIGS uses grid search over:\n- beam_size: {d/2, d} where d = number of features\n- min_imp_dec: constant search space (see Table V appendix)\n\nBest configuration trained on joint train+validation data; evaluated on held-out test set. 30 tuning iterations via hyperopt for baselines; grid search for RO-FIGS [14].\n\n### 4.6 odt_info Dictionary [15]\n\nPopulated after split construction:\n- features: numpy array of feature indices (excluding bias)\n- weights: numpy array from splitter.weights_bias (excluding bias)\n- threshold: splitter.threshold - splitter.weights_bias[-1]\n- n_samples_left: int\n- n_samples_right: int\n- error: bool (True if split failed)\n- one_node_only: bool (True if only root node)\n\n## 5. Synergy Graph Construction Algorithm\n\n### 5.1 Complete Algorithm\n\n```python\nimport networkx as nx\nimport numpy as np\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom collections import Counter\nimport dit\nfrom dit.pid import PID_BROJA\n\ndef build_synergy_graph(X, y, n_bins=5, threshold_percentile=75):\n    n_samples, d = X.shape\n    disc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n    X_disc = disc.fit_transform(X).astype(int)\n    y_disc = y.astype(int)\n\n    synergy_matrix = np.zeros((d, d))\n    for i in range(d):\n        for j in range(i+1, d):\n            synergy_matrix[i, j] = compute_pairwise_synergy(\n                X_disc[:, i], X_disc[:, j], y_disc)\n            synergy_matrix[j, i] = synergy_matrix[i, j]\n\n    positive_synergies = synergy_matrix[synergy_matrix > 0]\n    tau = np.percentile(positive_synergies, threshold_percentile) if len(positive_synergies) > 0 else 0.0\n\n    G = nx.Graph()\n    G.add_nodes_from(range(d))\n    for i in range(d):\n        for j in range(i+1, d):\n            if synergy_matrix[i, j] > tau:\n                G.add_edge(i, j, weight=synergy_matrix[i, j])\n\n    feature_subsets = []\n    for clique in nx.find_cliques(G):\n        if 2 <= len(clique) <= 5:\n            feature_subsets.append(sorted(clique))\n    for u, v in G.edges():\n        pair = sorted([u, v])\n        if pair not in feature_subsets:\n            feature_subsets.append(pair)\n\n    return G, synergy_matrix, feature_subsets\n```\n\n### 5.2 Threshold Strategies\n\nThree approaches:\n1. Percentile-based (recommended): tau = 75th percentile of positive synergy values. Adaptive to data distribution.\n2. Fixed absolute: tau = 0.01 bits. Simple but not adaptive.\n3. Permutation test (expensive): Shuffle target 100 times, compute null synergy distribution, use 95th percentile as threshold. ~100x slower.\n\n### 5.3 Clique Extraction [17]\n\nNetworkX find_cliques(G) returns all maximal cliques using the Bron-Kerbosch algorithm. Filter cliques by size: 2-5 suitable for oblique splits, >5 decompose into overlapping subsets of size 5.\n\n## 6. Complete SG-FIGS Algorithm Design\n\n### 6.1 Class Hierarchy\n\n```python\nclass SGFIGSClassifier(ROFIGSClassifier):\n    def __init__(self, beam_size=None, min_impurity_decrease=0.0,\n                 max_splits=75, max_trees=None, num_repetitions=5,\n                 n_bins=5, threshold_percentile=75,\n                 fallback_to_random=True, random_state=None):\n        super().__init__(beam_size=beam_size, ...)\n        self.n_bins = n_bins\n        self.threshold_percentile = threshold_percentile\n        self.fallback_to_random = fallback_to_random\n```\n\n### 6.2 Key Algorithmic Difference\n\nThe ONLY algorithmic difference between SG-FIGS and RO-FIGS is in feature subset selection:\n\n```python\n# RO-FIGS (line 4 of Algorithm 1):\nsplitting_features = random.sample(range(d), beam_size)\n\n# SG-FIGS:\nsplitting_features = self._select_synergy_guided_subset()\n```\n\nWhere _select_synergy_guided_subset: (1) Samples a synergy clique/edge from the pre-computed graph, (2) If clique size < beam_size: pads with random features, (3) If clique size > beam_size: selects top-synergy subset, (4) Fallback: random selection if no synergy subsets available.\n\n### 6.3 Axis-Aligned Fallback\n\nSG-FIGS should maintain axis-aligned splits as fallback by evaluating BOTH oblique (from synergy subset) and axis-aligned (original FIGS stump) splits, then choosing the one with higher impurity_reduction.\n\n## 7. Evaluation Protocol\n\n### 7.1 Datasets [14]\n\nRO-FIGS uses 22 binary classification datasets from OpenML: blood, diabetes, breast-w, ilpd, monks2, climate, kc2, pc1, kc1, heart, tictactoe, wdbc, churn, pc3, biodeg, credit, spambase, credit-g, friedman, usps, bioresponse, speeddating.\n\nFour priority datasets with known domain interactions:\n- diabetes (8 features): BMI-glucose, age-insulin\n- monks2 (6 features): XOR structure (pure synergy)\n- heart (13 features): chest_pain-exercise_angina\n- breast-w (9 features): texture-concavity\n\n### 7.2 Metrics [14]\n\n- Primary: Balanced accuracy on test split (mean +/- std across folds)\n- Model complexity: Number of splits, number of trees\n- Per-split complexity: Average features per split with non-zero weights\n- SG-FIGS specific: Fraction of oblique splits using above-median synergy pairs\n\n### 7.3 Statistical Testing [14]\n\n- 10-fold CV with OpenML-provided train/test splits\n- 80%/10%/10% train/val/test\n- Corrected Friedman test followed by Bonferroni-Dunn post-hoc test (p < 0.05)\n- Critical difference diagram for visual comparison\n\n### 7.4 Baselines [14]\n\nFrom RO-FIGS paper: DT, MT, OT, ODT, RF, ETC, CatBoost, FIGS, Ens-ODT, MLP, RO-FIGS.\n\nFor SG-FIGS, critical comparisons: (1) FIGS (axis-aligned), (2) RO-FIGS (random oblique, identical baseline), (3) SG-FIGS (proposed), (4) Ablation: random graph instead of synergy graph.\n\n## 8. RO-FIGS Baseline Implementation\n\n### 8.1 Official Code [13, 15]\n\nAvailable at https://github.com/um-k/rofigs.\n\nKey dependencies:\n- spyct (install: pip install git+https://gitlab.com/TStepi/spyct.git) [16]\n- numpy, scipy, scikit-learn, joblib\n- Requires C compiler (gcc) for spyct compilation\n\n### 8.2 Critical Implementation Notes\n\n1. Data must be min-max scaled to the zero-one range before passing to RO-FIGS [14]\n2. Categorical features must be one-hot encoded (E-ohe performs best) [14]\n3. SPyCT adds a bias column automatically in fit() [15]\n4. Worst-case complexity: O(i*r*m^2*n^2*d) where i=gradient iterations (100), r=repetitions (5), m=splits, n=samples, d=beam_size features [14]\n\n## 9. Critical Findings and Caveats\n\n### 9.1 PID_WB Over-estimates Synergy\n\nThe I_min measure (PID_WB) has a well-documented shortcoming: for distributions where inputs provide orthogonal information about the output, it misclassifies unique information as a mix of redundancy and synergy [3, 9]. PID_BROJA addresses this but is computationally more expensive.\n\n### 9.2 Discretization Sensitivity\n\nPID results are sensitive to the number of bins. Too few bins (B=2-3) loses information; too many bins (B>10) creates sparse contingency tables with unreliable probability estimates. Rule of thumb: ensure n_samples >> n_bins^3 for reliable PID estimates [10].\n\n### 9.3 SPyCT Required for Fair Comparison\n\nRO-FIGS uses SPyCT's gradient-based splitter because it directly optimizes impurity reduction with L_1/2 regularization. Using sklearn's RidgeClassifier as fallback provides L_2 regularization only (no sparsity). For fair comparison, SPyCT must be used [14, 16].\n\n### 9.4 Synergy Graph is One-Time Cost\n\nThe synergy graph construction is a one-time cost per fit() call. For d=50 features with B=5 bins, expect ~1-2 minutes. This is amortized over the entire FIGS training loop. The synergy computation does not need to be repeated during the greedy loop.",
  "sources": [
    {
      "index": 1,
      "url": "https://github.com/dit/dit",
      "title": "dit: Python package for discrete information theory (GitHub)",
      "summary": "Official repository for the dit library, confirming it as the primary Python package for information-theoretic computations including PID."
    },
    {
      "index": 2,
      "url": "https://dit.readthedocs.io/en/latest/",
      "title": "dit documentation — dit 1.2.3",
      "summary": "Official documentation homepage for dit, confirming library capabilities and version."
    },
    {
      "index": 3,
      "url": "https://dit.readthedocs.io/en/latest/measures/pid.html",
      "title": "Partial Information Decomposition — dit 1.2.3 documentation",
      "summary": "Core PID documentation with all PID measures (PID_WB, PID_BROJA, PID_CCS, etc.), lattice atom keys ({0:1} for synergy, {0}{1} for redundancy), code examples, and known criticisms of each measure."
    },
    {
      "index": 4,
      "url": "https://github.com/pietromarchesi/pidpy",
      "title": "pidpy: Python package for computing partial information decomposition",
      "summary": "pidpy repository showing binary-only feature support and triplet limitation for non-binary variables, making it unsuitable for discretized continuous features."
    },
    {
      "index": 5,
      "url": "https://arxiv.org/abs/2405.19212",
      "title": "Partial Information Decomposition for Data Interpretability and Feature Selection (AISTATS 2025)",
      "summary": "PIDF paper using neural mutual information estimation (MINE) for PID-based feature selection, relevant as alternative approach but using custom rather than classical PID."
    },
    {
      "index": 6,
      "url": "https://github.com/c-s-westphal/PIDF",
      "title": "PIDF Repository (GitHub)",
      "summary": "Implementation code for PIDF paper, showing custom neural-estimation approach rather than using dit or pidpy libraries."
    },
    {
      "index": 7,
      "url": "https://raw.githubusercontent.com/dit/dit/master/dit/pid/pid.py",
      "title": "dit PID source code — BasePID class",
      "summary": "Complete BasePID implementation showing get_pi() method with Moebius inversion, __getitem__ for dictionary-style access, and tuple-of-tuples node key format."
    },
    {
      "index": 8,
      "url": "https://dit.readthedocs.io/en/latest/distributions/npdist.html",
      "title": "Numpy-based Distribution — dit 1.2.3 documentation",
      "summary": "Documentation for creating dit Distribution objects from numpy arrays via from_ndarray() and from dictionary constructors."
    },
    {
      "index": 9,
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10969115/",
      "title": "A Measure of Synergy Based on Union Information (PMC)",
      "summary": "Academic comparison of PID measures showing PID_WB and PID_BROJA trade-offs, criticisms of each, and that no universal consensus exists on the best measure."
    },
    {
      "index": 10,
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html",
      "title": "KBinsDiscretizer — scikit-learn 1.8.0 documentation",
      "summary": "Official sklearn documentation for KBinsDiscretizer with quantile strategy for equal-frequency binning of continuous features."
    },
    {
      "index": 11,
      "url": "https://raw.githubusercontent.com/csinva/imodels/master/imodels/tree/figs.py",
      "title": "FIGS source code — imodels repository",
      "summary": "Complete FIGS implementation with Node class, _construct_node_with_stump method, fit() greedy loop with potential_splits and residual updates, and predict() tree traversal."
    },
    {
      "index": 13,
      "url": "https://raw.githubusercontent.com/um-k/rofigs/main/src/rofigs.py",
      "title": "RO-FIGS main source code (rofigs.py)",
      "summary": "Complete ROFIGS implementation showing Node class with features/weights/threshold, _construct_oblique_split, fit() with num_repetitions loop, and _predict_tree with dot-product oblique prediction."
    },
    {
      "index": 14,
      "url": "https://arxiv.org/html/2504.06927",
      "title": "RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data (IEEE 2025)",
      "summary": "Full RO-FIGS paper with Algorithm 1 pseudocode, L_1/2 regularization formula, 22 OpenML datasets, balanced accuracy results, corrected Friedman test, beam_size/min_imp_dec grid search, and computational complexity analysis."
    },
    {
      "index": 15,
      "url": "https://raw.githubusercontent.com/um-k/rofigs/main/src/odt.py",
      "title": "RO-FIGS ODT module source code (odt.py)",
      "summary": "Complete oblique decision tree wrapper around SPyCT's GradSplitter, showing splitting_features restriction, odt_info dict population, and GradSplitter initialization with all hyperparameters."
    },
    {
      "index": 16,
      "url": "https://github.com/knowledge-technologies/spyct",
      "title": "SPyCT: Python implementation of Oblique Predictive Clustering Trees (GitHub)",
      "summary": "Official SPyCT repository with Model class API, splitter options (grad/svm), regularization parameter C, learning rate, and installation instructions."
    },
    {
      "index": 17,
      "url": "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.clique.find_cliques.html",
      "title": "find_cliques — NetworkX documentation",
      "summary": "NetworkX maximal clique enumeration using Bron-Kerbosch algorithm, returning iterator over all maximal cliques."
    }
  ],
  "follow_up_questions": [
    "What is the empirical impact of different PID measures (PID_WB vs PID_BROJA vs PID_CCS) on the resulting synergy graph topology and downstream SG-FIGS performance — does the measure choice significantly affect which feature subsets are selected?",
    "How should SG-FIGS handle the cold-start problem where the synergy graph is computed on the full dataset but the FIGS greedy loop operates on residuals — should synergy be recomputed on residuals at each iteration, or is the initial synergy graph sufficient?",
    "Can the computational cost of pairwise PID be reduced for high-dimensional datasets (d>100) using approximate methods like mutual information pre-screening to prune obviously non-synergistic pairs before running full PID?"
  ]
}
